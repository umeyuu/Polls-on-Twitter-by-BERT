{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeda-yuusuke469/Polls-on-Twitter-by-BERT/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
    "from torch.utils.data import SequentialSampler\n",
    "from src.dataset import My_DATASET\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attentionの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "model_path = 'save_model/best_model.pth'\n",
    "data_path = 'DATA/serched_tweet/イーロンマスク.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/umeda-yuusuke469/Polls-on-Twitter-by-BERT/env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# モデルを読み込む\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, # 日本語Pre trainedモデルの指定\n",
    "        num_labels = 3, # ラベル数\n",
    "        output_attentions = False, # アテンションベクトルを出力するか\n",
    "        output_hidden_states = False, # 隠れ層を出力するか\n",
    "    )\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "df = pd.read_csv(data_path)\n",
    "df.columns = ['tweet']\n",
    "tweets = df.tweet.values.tolist()\n",
    "# データローダー\n",
    "dataset = My_DATASET(MODEL_NAME, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(word, attn):\n",
    "    html_color = '#%02X%02X%02X' % (255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
    "    return f'<span style=\"background-color: {html_color}\">{word}</span>'\n",
    "\n",
    "def id2label(id):\n",
    "    if id == 0:\n",
    "        return 'positive'\n",
    "    elif id == 1:\n",
    "        return 'negative'\n",
    "    elif id == 2:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'岸田首相もイーロンマスク習ってTwitterでアンケート取ればいいのに\\n\\n勝手に突っ走るから批判ばっかになるんだよ\\n\\n聞く耳あるなら身内じゃなくて民意に傾けろさ'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 8\n",
    "input_ids, input_mask = dataset[ind]\n",
    "output = model(input_ids.unsqueeze(0),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=input_mask.unsqueeze(0),\n",
    "            output_attentions=True)\n",
    "attention_weight = output.attentions[-1]\n",
    "id = output.logits.argmax(dim=1).item()\n",
    "label = id2label(id)\n",
    "# 文章の長さ分のzero tensorを宣言\n",
    "seq_len = attention_weight.size()[2]\n",
    "tmp = torch.zeros(seq_len)\n",
    "\n",
    "for i in range(12):\n",
    "    tmp += attention_weight[0, i, 0, :]\n",
    "\n",
    "all_attens = tmp[input_mask==1]\n",
    "att_max = all_attens.max().item()\n",
    "att_min = all_attens.min().item()\n",
    "all_attens = (all_attens-att_min)/(att_max-att_min)\n",
    "\n",
    "html = f'<big>推論ラベル：{label}</big><br>'\n",
    "for ids, attn in zip(input_ids, all_attens):\n",
    "    word = tokenizer.convert_ids_to_tokens([ids.numpy().tolist()])[0]\n",
    "    if word == \"[SEP]\":\n",
    "        break\n",
    "    html += highlight(word, attn)\n",
    "    # print(word, attn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<big>推論ラベル：negative</big><br><span style=\"background-color: #FF0B0B\">[CLS]</span><span style=\"background-color: #FFBABA\">岸田</span><span style=\"background-color: #FFDEDE\">首相</span><span style=\"background-color: #FFEBEB\">も</span><span style=\"background-color: #FFE2E2\">イー</span><span style=\"background-color: #FFE0E0\">##ロン</span><span style=\"background-color: #FFE8E8\">##マス</span><span style=\"background-color: #FFDEDE\">##ク</span><span style=\"background-color: #FF6464\">習っ</span><span style=\"background-color: #FFF5F5\">て</span><span style=\"background-color: #FFFCFC\">Twitter</span><span style=\"background-color: #FFF8F8\">で</span><span style=\"background-color: #FFDBDB\">アンケート</span><span style=\"background-color: #FFE9E9\">取れ</span><span style=\"background-color: #FFFBFB\">ば</span><span style=\"background-color: #FFC5C5\">いい</span><span style=\"background-color: #FFC8C8\">のに</span><span style=\"background-color: #FF8181\">勝手</span><span style=\"background-color: #FFF0F0\">に</span><span style=\"background-color: #FF7B7B\">突っ</span><span style=\"background-color: #FF8282\">##走</span><span style=\"background-color: #FF0000\">##る</span><span style=\"background-color: #FF9D9D\">から</span><span style=\"background-color: #FF5454\">批判</span><span style=\"background-color: #FFD5D5\">ば</span><span style=\"background-color: #FFEAEA\">##っか</span><span style=\"background-color: #FFF1F1\">に</span><span style=\"background-color: #FFC3C3\">なる</span><span style=\"background-color: #FFEDED\">ん</span><span style=\"background-color: #FF9797\">だ</span><span style=\"background-color: #FFC1C1\">よ</span><span style=\"background-color: #FFDFDF\">聞く</span><span style=\"background-color: #FFFCFC\">耳</span><span style=\"background-color: #FFFCFC\">ある</span><span style=\"background-color: #FFF4F4\">なら</span><span style=\"background-color: #FFFDFD\">身</span><span style=\"background-color: #FFFCFC\">##内</span><span style=\"background-color: #FFFEFE\">じゃ</span><span style=\"background-color: #FFFFFF\">なく</span><span style=\"background-color: #FFFEFE\">て</span><span style=\"background-color: #FFF7F7\">民</span><span style=\"background-color: #FFFDFD\">##意</span><span style=\"background-color: #FFFEFE\">に</span><span style=\"background-color: #FFE2E2\">傾け</span><span style=\"background-color: #FFDADA\">##ろ</span><span style=\"background-color: #FF8080\">さ</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fa68adc62e7031bae3ea93887c966b90b33c87a0f2a1bab09fad928a22685fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
